# -*- coding: utf-8 -*-
"""Demo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n0P4oMZINbOHxhUI6PFwBvPASft6nK-G

# 제반 작업
"""

import random

import torch
import os
import librosa
import demucs.separate
from pydub import AudioSegment
import malaya_speech
from pydub.silence import split_on_silence
import subprocess
import numpy as np
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
from IPython.display import Audio

"""# 모델 불러오기"""

class Extract_Model(nn.Module):
    def __init__(self):
        super(Extract_Model, self).__init__()#input.shape == batch,128,157
        self.conv0 = nn.Conv2d(1, 256, kernel_size=5, padding=2, padding_mode='zeros', stride=1)
        self.conv1 = nn.Conv2d(256, 256, kernel_size=5, padding=2, padding_mode='zeros', stride=1)
        self.conv2 = nn.Conv2d(256, 256, kernel_size=5, padding=2, padding_mode='zeros', stride=1)
        self.conv3 = nn.Conv2d(256, 256, kernel_size=3, padding=1, padding_mode='zeros', stride=1)
        self.conv4 = nn.Conv2d(256, 256, kernel_size=3, padding=1, padding_mode='zeros', stride=1)
        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, padding=1, padding_mode='zeros', stride=1)
        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=1, padding_mode='zeros', stride=1)
        self.pool = nn.MaxPool2d(2)
        self.fc0 = nn.Linear(8*9*256, 512)
        self.fc1 = nn.Linear(512, 256)
        self.relu = nn.ReLU()
        self.softmax = nn.Softmax()

    def forward(self, x):
        x = x.view(-1, 1, 128, 157)
        # x = (x/80)+1
        x = self.pool(self.relu(self.conv0(x)))#output.shape == 64,78
        y = x
        x = self.relu(self.conv1(x))#output.shape == 64,78
        x = self.relu(self.conv2(x))#output.shape == 64,78
        x = self.relu(self.conv3(x)+y)#output.shape == 64,78
        x = self.pool(self.relu(self.conv4(x)))#output.shape == 32,39
        x = self.pool(self.relu(self.conv5(x)))#output.shape == 16,19
        x = self.pool(self.relu(self.conv5(x)))#output.shape == 8,9
        x = torch.flatten(x,1)
        x = self.fc0(x)
        x = self.fc1(x)
        return x #output.shape == batch, 256

model_path = 'please.pt'
optimizer_path = 'optimizer_6_please.pt'

device = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu')

# 모델 불러오기
extract_model = Extract_Model()
extract_model.load_state_dict(torch.load(model_path))
extract_model.to(device)

"""# Utility"""

test_data = pd.read_csv('test_input_data 복사본.csv')
final_test_dataset = test_data.sort_values('name').reset_index().iloc[:,1:]
# final_test_dataset= final_test_dataset.iloc[[0,10,20,30,40,50,60,70,80,90]].reset_index().iloc[:,1:]
test_artist_list = final_test_dataset['name'].unique().tolist()

def timbre(input_data):
  output = extract_model(input_data)
  return output

def trim_mel(mel_spect):
    num_frames = mel_spect.shape[1]

    num_frames_5s = 157  # 5초 동안의 프레임 수 계산하기
    start_frame = np.random.randint(0, num_frames - num_frames_5s)
    mel_spect_5s = mel_spect[:, start_frame:start_frame + num_frames_5s]
    mel_spect_5s = mel_spect_5s / 80 + 1

    return mel_spect_5s

# 파일 형식 m4a에서 mp3로 변경 (아이폰 녹음의 경우 m4a로 저장됨)
def m4a_to_mp3(file_name):
  path = 'your_path'
  ffmpeg_cmd = f"ffmpeg -i '{path + file_name + '.m4a'}' '{path + file_name +'.mp3'}'"
  subprocess.call(ffmpeg_cmd, shell=True)
  y, sr = librosa.load(path + file_name + '.mp3', sr=16000)
  return y

def voice_preprocess(name):
  path = 'your_path'
  demucs.separate.main(["--mp3", "--two-stems", "vocals", "-n", "mdx_extra", path + name+ ".mp3"])
  y, sr = librosa.load(path + name + ".mp3", sr=16000)
  y_int = malaya_speech.astype.float_to_int(y)
  audio = AudioSegment(
          y_int.tobytes(),
          frame_rate = sr,
          sample_width = y_int.dtype.itemsize,
          channels = 1
      )
  audio_chunks = split_on_silence(
          audio,
          min_silence_len = 400,
          silence_thresh = -30,
          keep_silence = 400,
      )
  audio_chunks
  y_ = sum(audio_chunks)
  y_ = np.array(y_.get_array_of_samples())
  y_ = malaya_speech.astype.int_to_float(y_)
  spect = librosa.feature.melspectrogram(y=y_,sr=sr,n_fft=2048,hop_length=512, n_mels=128)
  mel_spect = librosa.power_to_db(S=spect, ref=np.max)
  np.save(path + name ,mel_spect)
  return mel_spect

def cut_and_concat_mp3(input_file, output_file, start_time_ms, end_time_ms):

    audio = AudioSegment.from_mp3(input_file)
    cut_audio = audio[start_time_ms:end_time_ms]
    cut_audio.export(output_file, format="mp3")

def voice_input(i):
  path = 'your_path'
  file_names = [file for file in os.listdir(path) if file.endswith('.npy')]
  file_name = file_names[i]
  curr_voice = file_name.split('.npy')[0]
  mel_spect = np.load(path+file_name, allow_pickle = True)
  mel_spect_5s = trim_mel(mel_spect)
  mel_spect_5s = torch.tensor(mel_spect_5s.reshape(1, 1, mel_spect_5s.shape[0], mel_spect_5s.shape[1])).to(device)

  return mel_spect_5s, curr_voice

def artist_input(i):
  path = 'your_path'
  # file_names = os.listdir(path)
  file_name = final_test_dataset['file'][i]
  curr_artist = file_name.split(' ', 1)[0]
  mel_spect = np.load(path+file_name, allow_pickle = True)
  mel_spect_5s = trim_mel(mel_spect)
  mel_spect_5s = torch.tensor(mel_spect_5s.reshape(1, 1, mel_spect_5s.shape[0], mel_spect_5s.shape[1])).to(device)

  return mel_spect_5s, curr_artist

def recommend(voice):
  similarities = []
  similarity_list = []
  for i in range(len(artist_timbre)):
      similarity = F.cosine_similarity(artist_timbre[i], voice, dim=1).item()
      if i%10 == 0:
        similarities.append(similarity_list)
        similarity_list = []
      similarity_list.append(similarity)

  most_similar_artist_index = similarities.index(max(similarities))
  most_similar_artist = artist_name[most_similar_artist_index]
  return most_similar_artist , max(similarities)

def recommend2(voice, threshold=0.7): # threshold 설정
    similarities = []
    similar_artists = []
    for idx, artist_output in enumerate(artist_timbre):
        similarity = F.cosine_similarity(artist_output, voice, dim=1).item()
        if similarity >= threshold:
            similarities.append(similarity)
            similar_artists.append(idx)

    if similarities:  # 유사도 리스트가 비어있지 않은 경우
        most_similar_artist_index = similar_artists[similarities.index(max(similarities))]
        most_similar_artist = artist_name[most_similar_artist_index]
        return most_similar_artist
    else:
        return None  # 유사한 아티스트가 없는 경우 None 반환

def recommend3(voice, threshold=0.7):
  similarities = []
  avg_list = []
  similar_artists = []

  for i in range(22):
    chunk = artist_timbre[i*10:(i+1)*10]
    avg = sum(chunk) / 10
    avg_list.append(avg)

  for idx, artist_output in enumerate(avg_list):
        similarity = F.cosine_similarity(artist_output, voice, dim=1).item()
        if similarity >= threshold:
            similarities.append(similarity)
            similar_artists.append(idx)


  if similarities:  # 유사도 리스트가 비어있지 않은 경우
        most_similar_artist_index = similarities.index(max(similarities))
        most_similar_artist = test_artist_list[most_similar_artist_index]
        return most_similar_artist , max(similarities)
  else:
        return None,None  # 유사한 아티스트가 없는 경우 None 반환

"""# 추론 및 음색 기반 가수 추천"""

artist_timbre = []
artist_name = []

for i in range(220):
    input_data, curr_artisit = artist_input(i)  # i번째 아티스트에 대한 입력 데이터 준비
    output = timbre(input_data)  # 모델 추론
    artist_name.append(curr_artisit)
    artist_timbre.append(output)

voice_name = []
voice_timbre = []

for i in range(3):
  input_data, file_name = voice_input(i)
  output = timbre(input_data)
  voice_timbre.append(output)
  voice_name.append(file_name)

voice0_recommend, voice0_cosine = recommend3(voice_timbre[0])
voice1_recommend, voice1_cosine = recommend3(voice_timbre[1])
voice2_recommend, voice2_cosine = recommend3(voice_timbre[2])

print(f'{voice_name[0]} 님과 가장 유사한 음색의 가수는 {voice0_recommend} 입니다. (유사도 : {voice0_cosine})')
print(f'{voice_name[1]} 님과 가장 유사한 음색의 가수는 {voice1_recommend} 입니다. (유사도 : {voice1_cosine})')
print(f'{voice_name[2]} 님과 가장 유사한 음색의 가수는 {voice2_recommend} 입니다. (유사도 : {voice2_cosine})')

# import pickle
#
# with open('/Users/seoeunseo/Desktop/deep.daiv/Audio/프로젝트/voice_timbre2(good_try).pickle', 'wb') as f:
#     pickle.dump(voice_timbre, f, pickle.HIGHEST_PROTOCOL)

"""# 결과 듣기"""

path = 'your_path' # music mp3 path

"""## 은"""

audio, sr = librosa.load('은.mp3', sr=16000)
Audio(data = audio, rate = sr)

song_name = final_test_dataset.iloc[np.random.choice(final_test_dataset[final_test_dataset['name'] == voice0_recommend].index)]['file'].replace('npy','mp3')

print(song_name.replace('.mp3',''))
audio_rec, sr = librosa.load(path + song_name, sr=16000)
Audio(data = audio_rec, rate = sr)

"""## 예"""

audio, sr = librosa.load('예.mp3', sr=16000)
Audio(data = audio, rate = sr)

"""## 민"""

audio, sr = librosa.load('민.mp3', sr=16000)
Audio(data = audio, rate = sr)

song_name = final_test_dataset.iloc[np.random.choice(final_test_dataset[final_test_dataset['name'] == voice2_recommend].index)]['file'].replace('npy','mp3')
print(song_name.replace('.mp3',''))
audio_rec, sr = librosa.load(path + song_name, sr=16000)
Audio(data = audio_rec, rate = sr)

"""# 참여용"""

def voice_input(file_name):
  path = 'your_path'
  curr_voice = file_name
  mel_spect = np.load(path+file_name+'.npy', allow_pickle = True)
  mel_spect_5s = trim_mel(mel_spect)
  mel_spect_5s = torch.tensor(mel_spect_5s.reshape(1, 1, mel_spect_5s.shape[0], mel_spect_5s.shape[1])).to(device)

  output = timbre(mel_spect_5s)
  voice_recommend, voice_cosine = recommend3(output)
  print(f'{curr_voice} 님과 가장 유사한 음색의 가수는 {voice_recommend} 입니다. (유사도 : {voice_cosine})')

voice_input('새로운 녹음')